#!/usr/bin/env python3
"""
Upload Audio Dataset to Huggingface Hub

This script uploads the audio dataset generated by generate_audio.py to the Huggingface Hub.
It processes the audio_dataset.csv file and creates a proper Huggingface dataset with audio files.
"""

import csv
import os
import argparse
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
from datasets import Dataset, Audio, DatasetDict
from huggingface_hub import HfApi, login
import json
from tqdm import tqdm


class HuggingfaceUploader:
    def __init__(
        self,
        dataset_name: str,
        organization: str = None,
        private: bool = False,
        audio_column: str = "audio",
        sampling_rate: int = 16000
    ):
        """
        Initialize the Huggingface uploader.

        Args:
            dataset_name: Name of the dataset on Huggingface Hub
            organization: Organization name (optional)
            private: Whether to make the dataset private
            audio_column: Name of the audio column in the dataset
            sampling_rate: Sampling rate for audio files
        """
        self.dataset_name = dataset_name
        self.organization = organization
        self.private = private
        self.audio_column = audio_column
        self.sampling_rate = sampling_rate
        
        # Create full dataset identifier
        if organization:
            self.dataset_id = f"{organization}/{dataset_name}"
        else:
            self.dataset_id = dataset_name
        
        print(f"Dataset ID: {self.dataset_id}")
        print(f"Private: {private}")
        print(f"Sampling rate: {sampling_rate} Hz")

    def load_dataset_from_csv(self, csv_path: str) -> Dataset:
        """
        Load the dataset from the CSV file generated by generate_audio.py.

        Args:
            csv_path: Path to the audio_dataset.csv file

        Returns:
            Dataset: Huggingface dataset with audio and metadata
        """
        print(f"Loading dataset from: {csv_path}")
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSV file not found: {csv_path}")
        
        # Read the CSV file
        df = pd.read_csv(csv_path)
        print(f"Loaded {len(df)} rows from CSV")
        
        # Validate required columns
        required_columns = ['audio_path', 'original_sentence', 'clean_sentence', 'class_label']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        # Check if audio files exist
        missing_files = []
        for idx, row in df.iterrows():
            audio_path = row['audio_path']
            if not os.path.exists(audio_path):
                missing_files.append(audio_path)
        
        if missing_files:
            print(f"Warning: {len(missing_files)} audio files are missing!")
            print("First 5 missing files:")
            for file in missing_files[:5]:
                print(f"  - {file}")
            
            # Remove rows with missing audio files
            df = df[df['audio_path'].apply(os.path.exists)]
            print(f"Filtered to {len(df)} rows with existing audio files")
        
        # Convert DataFrame to Dataset
        dataset = Dataset.from_pandas(df)
        
        # Add audio column with proper Audio feature
        dataset = dataset.cast_column(
            "audio_path", 
            Audio(sampling_rate=self.sampling_rate)
        )
        
        # Rename audio_path to audio for consistency
        dataset = dataset.rename_column("audio_path", self.audio_column)
        
        print(f"Dataset created with {len(dataset)} samples")
        return dataset

    def create_dataset_card(self, dataset: Dataset) -> str:
        """
        Create a dataset card (README) for the dataset.

        Args:
            dataset: The dataset to create a card for

        Returns:
            str: Dataset card content in markdown format
        """
        # Get class distribution
        class_counts = {}
        for item in dataset:
            label = item['class_label']
            class_counts[label] = class_counts.get(label, 0) + 1
        
        # Create dataset card content
        card_content = f"""# {self.dataset_name.replace('_', ' ').title()}

## Dataset Description

This dataset contains audio commands with voice control labels generated using text-to-speech synthesis.

### Dataset Summary

- **Total samples**: {len(dataset)}
- **Number of classes**: {len(class_counts)}
- **Audio format**: MP3, 16kHz sampling rate
- **Text-to-speech engine**: Google Text-to-Speech (gTTS)

### Class Distribution

| Class Label | Count |
|-------------|--------|
"""
        
        for label, count in sorted(class_counts.items()):
            card_content += f"| {label} | {count} |\n"
        
        card_content += f"""
### Dataset Structure

#### Data Fields

- `{self.audio_column}` (Audio): Audio file containing the spoken command
- `original_sentence` (string): Original sentence with special tokens
- `clean_sentence` (string): Clean sentence without special tokens (used for TTS)
- `class_label` (string): Command class label (e.g., `<volume_up>`, `<volume_down>`)
- `info` (string): Additional information about the audio generation

#### Example

```python
{{
    '{self.audio_column}': {{'path': 'audio_0001_volume_up_16k.mp3', 'array': [...], 'sampling_rate': 16000}},
    'original_sentence': 'Please <volume_up> the music',
    'clean_sentence': 'Please the music',
    'class_label': '<volume_up>',
    'info': 'gTTS, 16kHz mp3'
}}
```

### Usage

```python
from datasets import load_dataset

# Load the dataset
dataset = load_dataset("{self.dataset_id}")

# Access audio and labels
for sample in dataset["train"]:
    audio = sample["{self.audio_column}"]
    label = sample["class_label"]
    clean_text = sample["clean_sentence"]
    print(f"Label: {{label}}, Text: {{clean_text}}")
```

### Dataset Creation

This dataset was created using the `generate_audio.py` script, which:
1. Processes command sentences containing special tokens
2. Removes special tokens to create clean sentences
3. Generates audio using Google Text-to-Speech (gTTS)
4. Resamples audio to 16kHz for consistency

### Licensing

Please refer to the original dataset license and Google's terms of service for gTTS usage.
"""
        
        return card_content

    def upload_dataset(
        self, 
        csv_path: str, 
        split_name: str = "train",
        create_splits: bool = False,
        test_size: float = 0.2,
        validation_size: float = 0.1
    ) -> None:
        """
        Upload the dataset to Huggingface Hub.

        Args:
            csv_path: Path to the audio_dataset.csv file
            split_name: Name of the split if not creating multiple splits
            create_splits: Whether to create train/test/validation splits
            test_size: Proportion of data for test split
            validation_size: Proportion of data for validation split
        """
        print("=" * 60)
        print("UPLOADING DATASET TO HUGGINGFACE HUB")
        print("=" * 60)
        
        # Load the dataset
        dataset = self.load_dataset_from_csv(csv_path)
        
        # Create splits if requested
        if create_splits:
            print(f"Creating splits: train/validation/test")
            print(f"Test size: {test_size}, Validation size: {validation_size}")
            
            # First split: train + temp
            train_test_split = dataset.train_test_split(test_size=test_size, seed=42)
            
            # Second split: temp -> validation + test
            val_test_split = train_test_split['test'].train_test_split(
                test_size=validation_size/(test_size), seed=42
            )
            
            dataset_dict = DatasetDict({
                'train': train_test_split['train'],
                'validation': val_test_split['train'],
                'test': val_test_split['test']
            })
            
            print(f"Split sizes:")
            for split_name, split_data in dataset_dict.items():
                print(f"  {split_name}: {len(split_data)} samples")
        else:
            dataset_dict = DatasetDict({split_name: dataset})
            print(f"Using single split: {split_name} ({len(dataset)} samples)")
        
        # Create dataset card
        print("Creating dataset card...")
        card_content = self.create_dataset_card(dataset)
        
        # Upload to Huggingface Hub
        print(f"Uploading to: {self.dataset_id}")
        print("This may take a while depending on the size of your audio files...")
        
        try:
            dataset_dict.push_to_hub(
                self.dataset_id,
                private=self.private,
                commit_message="Upload audio command dataset"
            )
            
            # Upload dataset card
            api = HfApi()
            api.upload_file(
                path_or_fileobj=card_content.encode(),
                path_in_repo="README.md",
                repo_id=self.dataset_id,
                repo_type="dataset",
                commit_message="Add dataset card"
            )
            
            print("‚úÖ Dataset uploaded successfully!")
            print(f"üîó Dataset URL: https://huggingface.co/datasets/{self.dataset_id}")
            
        except Exception as e:
            print(f"‚ùå Error uploading dataset: {e}")
            raise


def main():
    parser = argparse.ArgumentParser(description='Upload audio dataset to Huggingface Hub')
    
    # Required arguments
    parser.add_argument('dataset_name', 
                       help='Name of the dataset on Huggingface Hub')
    
    # Optional arguments
    parser.add_argument('--csv', '-c', default='audio_dataset.csv',
                       help='Path to the audio dataset CSV file (default: audio_dataset.csv)')
    parser.add_argument('--organization', '-o', default=None,
                       help='Organization name on Huggingface Hub')
    parser.add_argument('--private', action='store_true',
                       help='Make the dataset private')
    parser.add_argument('--split-name', default='train',
                       help='Name of the split if not creating multiple splits (default: train)')
    parser.add_argument('--create-splits', action='store_true',
                       help='Create train/validation/test splits')
    parser.add_argument('--test-size', type=float, default=0.2,
                       help='Proportion of data for test split (default: 0.2)')
    parser.add_argument('--validation-size', type=float, default=0.1,
                       help='Proportion of data for validation split (default: 0.1)')
    parser.add_argument('--sampling-rate', type=int, default=16000,
                       help='Audio sampling rate (default: 16000)')
    
    args = parser.parse_args()
    
    # Validate arguments
    if not os.path.exists(args.csv):
        print(f"Error: CSV file not found: {args.csv}")
        print("Make sure to run generate_audio.py first to create the dataset!")
        return
    
    if args.create_splits:
        if args.test_size + args.validation_size >= 1.0:
            print("Error: test_size + validation_size must be less than 1.0")
            return
    
    # Check if user is logged in to Huggingface
    try:
        api = HfApi()
        user = api.whoami()
        print(f"Logged in as: {user['name']}")
    except Exception:
        print("Please login to Huggingface Hub first:")
        print("Run: huggingface-cli login")
        return
    
    # Create uploader and upload dataset
    uploader = HuggingfaceUploader(
        dataset_name=args.dataset_name,
        organization=args.organization,
        private=args.private,
        sampling_rate=args.sampling_rate
    )
    
    uploader.upload_dataset(
        csv_path=args.csv,
        split_name=args.split_name,
        create_splits=args.create_splits,
        test_size=args.test_size,
        validation_size=args.validation_size
    )


if __name__ == "__main__":
    main() 